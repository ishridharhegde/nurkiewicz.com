---
layout: post
title: Tenfold increase in server throughput with Servlet 3.0 asynchronous processing
date: '2011-03-11T23:22:00.004+01:00'
author: Tomasz Nurkiewicz
tags:
- servlets
- jmeter
- performance
- spring
- jmx
- monitoring
- tomcat
modified_time: '2011-11-17T19:14:40.640+01:00'
thumbnail: https://lh5.googleusercontent.com/-uKjs32ELGeY/TXqdMttwQiI/AAAAAAAAAZ0/9JCcQtXSyUM/s72-c/Active+Threads+Over+Time.png
blogger_id: tag:blogger.com,1999:blog-6753769565491687768.post-1457220125559134237
blogger_orig_url: https://www.nurkiewicz.com/2011/03/tenfold-increase-in-server-throughput.html
---

<style type="text/css">p { margin-bottom: 0.21cm; }h5 { margin-bottom: 0.21cm; page-break-after: avoid; }h5.western { font-family: "Arial",sans-serif; font-size: 11pt; font-weight: bold; }h5.cjk { font-family: "DejaVu Sans"; font-size: 11pt; font-weight: bold; }h5.ctl { font-family: "Lohit Hindi"; font-size: 11pt; font-weight: bold; }a:link { color: rgb(0, 0, 128); text-decoration: underline; } </style>  <br /><div style="margin-bottom: 0cm;">It is not a secret that Java servlet containers aren't particularly suited for handling large amount of concurrent users. Commonly established thread-per-request model effectively limits the number of concurrent connections to the number of concurrently running threads JVM can handle. And because every new thread introduces significant increase of memory footprint and CPU utilization (context switches), handling more than 100-200 concurrent connections seems like a ridiculous idea in Java. At least it was in pre-Servlet 3.0 era.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">In this article we will write scalable and robust file download server with throttled speed limit. Second version, leveraging Servlet 3.0 asynchronous processing feature, will be able to handle <b>ten times bigger load using even less threads</b>. No additional hardware required, just few wise design decisions.</div><div style="margin-bottom: 0cm;"><br /></div><h5 class="western">Token bucket algorithm</h5><div style="margin-bottom: 0cm;"><span style="font-weight: normal;">Building a file download servers we have to consciously manage are our resources, especially network bandwidth. We don't want a single client to consume the whole traffic, we might even want to throttle the download limit dynamically at runtime, based on user, time of the day, etc. - and of course everything happens during heavy load. Developers love reinventing the wheel, </span><span style="font-weight: normal;">unfortunately all our requirements are already addressed by absurdly simple <a href="http://en.wikipedia.org/wiki/Token_bucket">token bucket algorithm</a>.</span></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The explanation in Wikipedia is pretty good, but since we'll adjust the algorithm a bit for our needs, here's even simpler description. First there was a bucket. In this bucket there were uniform tokens. Each token is worth 20 kiB (I will be using real values from our application) of raw data. Every time a client ask for a file, the server tries to take one token from the bucket. If it succeeds, he sends 20 kiB to the client. Repeat last two sentences. What if the server fails to obtain the token because the bucket is already empty? He waits.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><a href="http://draft.blogger.com/post-edit.g?blogID=6753769565491687768&amp;postID=1457220125559134237" name="__DdeLink__0_377059792"></a>So where are the tokens coming from? Background process fill the bucket from time to time. Now it becomes clear. If this background process adds 100 new tokens every 100 ms (10 times per second), each worth 20 kiB, the server is capable of sending 20 MiB/s (100 times 20 kiB times 10) max, shared amongst all the clients. Of course if the bucket is full (1000 tokens), new tokens are ignored. This works amazingly well – if bucket is empty, clients are waiting for next bucket filling cycle; and by controlling the bucket capacity we can limit total bandwidth.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Enough of talking, our simplistic implementation of token bucket starts with an interface (whole source code is available on <a href="https://github.com/nurkiewicz/token-bucket">GitHub</a> in <a href="https://github.com/nurkiewicz/token-bucket/blob/global-bucket">global-bucket</a> branch):<br /><br /><a name='more'></a></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />public interface TokenBucket {<br /><br />    int TOKEN_PERMIT_SIZE = 1024 * 20;<br /><br />    void takeBlocking() throws InterruptedException;<br />    void takeBlocking(int howMany) throws InterruptedException;<br /><br />    boolean tryTake();<br />    boolean tryTake(int howMany);<br /><br />}<br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">takeBlocking()</span> methods are waiting synchronously for the token to become available, while <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">tryTake()</span> are taking token only if it is available, returning true immediately if taken, false otherwise. Fortunately the term bucket is just an abstraction: because tokens are indistinguishable, all we need to implement bucket is an integer counter. But because the bucket is inherently multi-threaded and some waiting is involved, we need more sophisticated mechanism. <a href="http://download.oracle.com/javase/6/docs/api/java/util/concurrent/Semaphore.html" style="font-family: &quot;Courier New&quot;,Courier,monospace;">Semaphore</a> seems to be almost ideal:</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><a href="http://draft.blogger.com/post-edit.g?blogID=6753769565491687768&amp;postID=1457220125559134237" name="__DdeLink__2_377059792"></a><pre class="brush: java"><br />@Service<br />@ManagedResource<br />public class GlobalTokenBucket extends TokenBucketSupport {<br /><br />    private final Semaphore bucketSize = new Semaphore(0, false);<br /><br />    private volatile int bucketCapacity = 1000;<br /><br />    public static final int BUCKET_FILLS_PER_SECOND = 10;<br /><br />    @Override<br />    public void takeBlocking(int howMany) throws InterruptedException {<br />        bucketSize.acquire(howMany);<br />    }<br /><br />    @Override<br />    public boolean tryTake(int howMany) {<br />        return bucketSize.tryAcquire(howMany);<br />    }<br /><br />}<br /><br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">Semaphore</span> fits exactly to our requirements. <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">bucketSize</span> represents current amount of tokens in the bucket. <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">bucketCapacity</span> on the other hand limits the bucket maximum size. It is volatile because it can be modified via JMX (visibility):</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />@ManagedAttribute<br />public int getBucketCapacity() {<br />    return bucketCapacity;<br />}<br /><br />@ManagedAttribute<br />public void setBucketCapacity(int bucketCapacity) {<br />    isTrue(bucketCapacity &gt;= 0);<br />    this.bucketCapacity = bucketCapacity;<br />}<br /><br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">As you can see I am using Spring and its support for JMX. Spring framework isn't absolutely necessary in this application, but it brings some nice features. For instance implementing a background process that periodically fills the bucket looks like this:</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />@Scheduled(fixedRate = 1000 / BUCKET_FILLS_PER_SECOND)<br />public void fillBucket() {<br />    final int releaseCount = min(bucketCapacity / BUCKET_FILLS_PER_SECOND, bucketCapacity - bucketSize.availablePermits());<br />    bucketSize.release(releaseCount);<br />}<br /><br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">For the record: (1) <a href="https://github.com/nurkiewicz/token-bucket/blob/global-bucket/src/main/webapp/WEB-INF/applicationContext.xml">tiny</a> XML snippet is required to make <a href="http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/scheduling.html#scheduling-annotation-support-scheduled" style="font-family: &quot;Courier New&quot;,Courier,monospace;">@Scheduled</a> annotation working and (2) – this code contains major multi-threading bug that we can ignore for the purposes of this article. It is suppose to fill the bucket up to the maximum value – will it always work?</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Having token bucket abstraction and very basic implementation we can develop the actual servlet returning files. I am always returning the same fixed file with size of almost 200 kiB):</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />@WebServlet(urlPatterns = "/*", name="downloadServletHandler")<br />public class DownloadServlet extends HttpRequestHandlerServlet {}<br /><br /><br />@Service<br />public class DownloadServletHandler implements HttpRequestHandler {<br /><br />    private static final Logger log = LoggerFactory.getLogger(DownloadServletHandler.class);<br /><br />    @Resource<br />    private TokenBucket tokenBucket;<br /><br />    @Override<br />    public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {<br />        final File file = new File("/home/dev/tmp/ehcache-1.6.2.jar");<br />        final BufferedInputStream input = new BufferedInputStream(new FileInputStream(file));<br />        try {<br />            response.setContentLength((int) file.length());<br />            sendFile(request, response, input);<br />        } catch (InterruptedException e) {<br />            log.error("Download interrupted", e);<br />        } finally {<br />            input.close();<br />        }<br /><br />    }<br /><br />    private void sendFile(HttpServletRequest request, HttpServletResponse response, BufferedInputStream input) throws IOException, InterruptedException {<br />        byte[] buffer = new byte[TokenBucket.TOKEN_PERMIT_SIZE];<br />        final ServletOutputStream outputStream = response.getOutputStream();<br />        for (int count = input.read(buffer); count &gt; 0; count = input.read(buffer)) {<br />            tokenBucket.takeBlocking();<br />            outputStream.write(buffer, 0, count);<br />        }<br />    }<br />}<br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><a href="http://static.springsource.org/spring/docs/3.0.x/javadoc-api/org/springframework/web/context/support/HttpRequestHandlerServlet.html" style="font-family: &quot;Courier New&quot;,Courier,monospace;">HttpRequestHandlerServlet</a> was used here. As simple as can be: read 20 kiB of file, take the token from the bucket (waiting if unavailable), send chunk to the client, repeat until the end of file.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Believe it or not, this actually works! No matter how many (or how few) clients are concurrently accessing this servlet, total outgoing network bandwidth never exceeds 20 MiB! The algorithm works and I hope you get some basic feeling how to use it. But let's face it – global limit is way too inflexible and kind of lame – single client can actually consume your whole bandwidth.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">So what if we had a separate bucket for each client? Instead of one semaphore – a map? Each client has a separate independent bandwidth limit, so there is no risk of starvation. But there is even more:  </div><ul><li><div style="margin-bottom: 0cm;">some clients might be more  privileged, having bigger or no limit at all,</div></li><li><div style="margin-bottom: 0cm;">some might be black listed,  resulting in connection rejection or very low throughput</div></li><li><div style="margin-bottom: 0cm;">banning IPs, requiring  authentication, cookie/user agent verification, etc.</div></li><li><div style="margin-bottom: 0cm;">we might try to correlate  concurrent requests coming from the same client and use the same  bucket for all of them to avoid cheating by opening several  connections. We might also reject subsequent connections</div></li><li><div style="margin-bottom: 0cm;">and much more...</div></li></ul><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Our bucket interface grows allowing the implementation to take advantage of the new possibilities (see branch <a href="https://github.com/nurkiewicz/token-bucket/tree/per-request-synch">per-request-synch</a>):</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />public interface TokenBucket {<br /><br />    void takeBlocking(ServletRequest req) throws InterruptedException;<br />    void takeBlocking(ServletRequest req, int howMany) throws InterruptedException;<br /><br />    boolean tryTake(ServletRequest req);<br />    boolean tryTake(ServletRequest req, int howMany);<br /><br />    void completed(ServletRequest req);<br />}<br /><br /><br /><br />public class PerRequestTokenBucket extends TokenBucketSupport {<br /><br />    private final ConcurrentMap&lt;Long, Semaphore &gt; bucketSizeByRequestNo = new ConcurrentHashMap&lt;Long, Semaphore &gt;();<br /><br />    @Override<br />    public void takeBlocking(ServletRequest req, int howMany) throws InterruptedException {<br />        getCount(req).acquire(howMany);<br />    }<br /><br />    @Override<br />    public boolean tryTake(ServletRequest req, int howMany) {<br />        return getCount(req).tryAcquire(howMany);<br />    }<br /><br />    @Override<br />    public void completed(ServletRequest req) {<br />        bucketSizeByRequestNo.remove(getRequestNo(req));<br />    }<br /><br />    private Semaphore getCount(ServletRequest req) {<br />        final Semaphore semaphore = bucketSizeByRequestNo.get(getRequestNo(req));<br />        if (semaphore == null) {<br />            final Semaphore newSemaphore = new Semaphore(0, false);<br />            bucketSizeByRequestNo.putIfAbsent(getRequestNo(req), newSemaphore);<br />            return newSemaphore;<br />        } else {<br />            return semaphore;<br />        }<br />    }<br /><br />    private Long getRequestNo(ServletRequest req) {<br />        final Long reqNo = (Long) req.getAttribute(REQUEST_NO);<br />        if (reqNo == null) {<br />            throw new IllegalAccessError("Request # not found in: " + req);<br />        }<br />        return reqNo;<br />    }<br /><br />}<br /><br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The implementation is very similar (full class <a href="https://github.com/nurkiewicz/token-bucket/blob/per-request-synch/src/main/java/com/blogspot/nurkiewicz/download/DownloadServletHandler.java">here</a>) except that the single semaphore was replaced by map. I am not using request object itself as a map key for various reasons but a unique request number that I am assigning manually when receiving new connection. Calling completed() is very important, otherwise the map would grow continuously leading to memory leak. All in all, the token bucket implementation haven't changed a lot, also the download servlet is almost the same (except passing request to token bucket). We are now ready for some stress testing!</div><div style="margin-bottom: 0cm;"><br /></div><h5 class="western">Throughput testing</h5><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">For the testing purposes we will use <a href="http://jakarta.apache.org/jmeter/">JMeter</a> with this wonderful set of <a href="http://code.google.com/p/jmeter-plugins/">plugins</a>. During the 20-minute testing session we warm up our server firing up one new thread (concurrent connection) every 6 seconds to reach 100 threads after 10 minutes. For the next ten minutes we will keep 100 concurrent connections to see how stable the server works:</div><div style="margin-bottom: 0cm;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://lh5.googleusercontent.com/-uKjs32ELGeY/TXqdMttwQiI/AAAAAAAAAZ0/9JCcQtXSyUM/s1600/Active+Threads+Over+Time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="197" src="https://lh5.googleusercontent.com/-uKjs32ELGeY/TXqdMttwQiI/AAAAAAAAAZ0/9JCcQtXSyUM/s320/Active+Threads+Over+Time.png" width="320" /></a></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>Important note</b>: I artificially lowered the number of HTTP worker threads to <b>10</b> in Tomcat (7.0.10 tested). This is a far from real configuration, but I wanted to emphasize some phenomena that occur with high load compared to server capabilities. With default pool size I would need several client machines running distributed JMeter session to generate enough traffic. If you have a server farm or couple of servers in the cloud (as opposed to my 3-year-old laptop), I would be delighted to see the results in more realistic environment.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Remembering how many HTTP worker threads are available in Tomcat, response times over time are far from satisfactory:</div><div style="margin-bottom: 0cm;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://lh6.googleusercontent.com/--CWo69ehvOw/TXqdYbf-xdI/AAAAAAAAAZ4/2J3BTeW5Kcs/s1600/Response+Times+Over+Time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="197" src="https://lh6.googleusercontent.com/--CWo69ehvOw/TXqdYbf-xdI/AAAAAAAAAZ4/2J3BTeW5Kcs/s320/Response+Times+Over+Time.png" width="320" /></a></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Please note the plateau at the beginning of the test: after about a minute (hint: when the number of concurrent connections exceeds 10) response times are skyrocketing to stabilize at around 10 seconds after 10 minutes (number of concurrent connections reaches one hundred). Once again: the same behavior would occur with 100 worker threads and 1000 concurrent connections – it's just a matter of scale. The response latencies graph (time between sending request and receiving first lines of response) clears any doubts:</div><div style="margin-bottom: 0cm;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://lh6.googleusercontent.com/-mkl08vVSROM/TXqdkbc6-yI/AAAAAAAAAZ8/cXi15QU6rqE/s1600/Response+Latencies+Over+Time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="197" src="https://lh6.googleusercontent.com/-mkl08vVSROM/TXqdkbc6-yI/AAAAAAAAAZ8/cXi15QU6rqE/s320/Response+Latencies+Over+Time.png" width="320" /></a></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Below magical 10 threads our application responds almost instantly. This is really important for clients as receiving only headers  (especially <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">Content-Type</span> and <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">Content-Length</span>) allows them to more accurately inform the user what is going on. So what is the reason of Tomcat waiting with the response? No magic here really. We have only 10 threads and each connection requires one thread, so Tomcat (and any other pre-Servlet 3.0 container) handles 10 clients while the remaining 90 are... queued. The moment one of the 10 lucky ones is done, one connection from the queue is taken. This explains average 9 second latency whilst the servlet needs only 1 second to serve the request (200 kiB with 20 kiB/s limit). If you are still not convinced, Tomcat provides nice JMX indicators showing how many threads are occupied and how many requests are queued:</div><div style="margin-bottom: 0cm;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://lh4.googleusercontent.com/-9o9aY_HxOVk/TXqdqoF_wFI/AAAAAAAAAaA/sTAlBHf1zKY/s1600/zrzut_ekranu-1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="220" src="https://lh4.googleusercontent.com/-9o9aY_HxOVk/TXqdqoF_wFI/AAAAAAAAAaA/sTAlBHf1zKY/s320/zrzut_ekranu-1.png" width="320" /></a></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">With traditional servlets there is nothing we can do. Throughput is horrible but increasing the total number of threads is not an option (think: from 100 to 1000). But you don't actually need a profiler to discover that threads aren't the true bottleneck here. Look carefully at DownloadServletHandler, where do you think most of the time is spent? Reading a file? Sending data back to the client? No, the servlet waits... And then waits even more. Non-productively hanging on semaphore – thankfully CPU is not harmed, but what if it was implemented using busy waiting? Luckily Tomcat 7 finally supports...</div><div style="margin-bottom: 0cm;"><br /></div><h5 class="western">Servlet 3.0 asynchronous processing</h5><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">We are <i>this</i><span style="font-style: normal;"> close to increase our server capacity by </span><span style="font-style: normal;">an order of magnitude, but some non-trivial changes are required (see <a href="https://github.com/nurkiewicz/token-bucket">master</a> branch). First, download servlet needs to be marked as asynchronous (OK, this is </span><span style="font-style: normal;">still </span><span style="font-style: normal;">trivial):</span></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><span style="font-style: normal;"><pre class="brush: java"><br />@WebServlet(urlPatterns = "/*", name="downloadServletHandler", asyncSupported = true)<br />public class DownloadServlet extends HttpRequestHandlerServlet {}<br /><br /></pre></span></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The main change occurs in download handler. Instead of sending the whole file in a loop with lots of waiting (<span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">takeBlocking()</span>) involved, we are splitting the loop into separate iterations, each wrapped inside <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">Callable</span>. Now we will utilize a small thread pool that will be shared by all awaiting connections. Each task in the pool is very simple: instead of waiting for a token, it asks for it in a non-blocking fashion (<span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">tryTake()</span>). If the token is available, piece of the file is sent to the client (<span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">sendChunkWorthOneToken()</span>). If the token is not available (bucket is empty), nothing happens. No matter whether the token was available or not, the task resubmits itself to the queue for further processing (this is essentially very fancy, multi-threaded loop). Because there is only one pool, the task lands at the end of the queue allowing other connections to be served.  </div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: java"><br />@Service<br />public class DownloadServletHandler implements HttpRequestHandler {<br /><br />    @Resource<br />    private TokenBucket tokenBucket;<br /><br />    @Resource<br />    private ThreadPoolTaskExecutor downloadWorkersPool;<br /><br />    @Override<br />    public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {<br />        final File file = new File("/home/dev/tmp/ehcache-1.6.2.jar");<br />        response.setContentLength((int) file.length());<br />        final BufferedInputStream input = new BufferedInputStream(new FileInputStream(file));<br />        final AsyncContext asyncContext = request.startAsync(request, response);<br />        downloadWorkersPool.submit(new DownloadChunkTask(asyncContext, input));<br />    }<br /><br />    private class DownloadChunkTask implements Callable&lt;Void&gt; {<br /><br />        private final BufferedInputStream fileInputStream;<br />        private final byte[] buffer = new byte[TokenBucket.TOKEN_PERMIT_SIZE];<br />        private final AsyncContext ctx;<br /><br />        public DownloadChunkTask(AsyncContext ctx, BufferedInputStream fileInputStream) throws IOException {<br />            this.ctx = ctx;<br />            this.fileInputStream = fileInputStream;<br />        }<br /><br />        @Override<br />        public Void call() throws Exception {<br />            try {<br />                if (tokenBucket.tryTake(ctx.getRequest())) {<br />                    sendChunkWorthOneToken();<br />                } else<br />                    downloadWorkersPool.submit(this);<br />            } catch (Exception e) {<br />                log.error("", e);<br />                done();<br />            }<br />            return null;<br />        }<br /><br />        private void sendChunkWorthOneToken() throws IOException {<br />            final int bytesCount = fileInputStream.read(buffer);<br />            ctx.getResponse().getOutputStream().write(buffer, 0, bytesCount);<br />            if (bytesCount &lt; buffer.length)<br />                done();<br />            else<br />                downloadWorkersPool.submit(this);<br />        }<br /><br />        private void done() throws IOException {<br />            fileInputStream.close();<br />            tokenBucket.completed(ctx.getRequest());<br />            ctx.complete();<br />        }<br />    }<br /><br />}<br /><br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">I am leaving the details of Servlet 3.0 API, there are plenty of less sophisticated examples throughout the Internet. Just remember to call <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">startAsync()</span> and work with returned <a href="http://download.oracle.com/javaee/6/api/javax/servlet/AsyncContext.html"><span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">AsyncContext</span></a> instead of plain request and response.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">BTW creating a thread pool using Spring is childishly easy (and we get nice thread names as opposed to <a href="http://download.oracle.com/javase/6/docs/api/java/util/concurrent/Executors.html" style="font-family: &quot;Courier New&quot;,Courier,monospace;">Executors</a> and <a href="http://download.oracle.com/javase/6/docs/api/java/util/concurrent/ExecutorService.html" style="font-family: &quot;Courier New&quot;,Courier,monospace;">ExecutorService</a>):</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><pre class="brush: xml"><br />&lt;task:executor id="downloadWorkersPool" pool-size="1"/&gt;<br /></pre></div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">That's right, one thread is enough to serve one hundred concurrent clients. See for yourself (the amount of HTTP worker threads is still 10 and yes, the scale is in milliseconds):</div><div style="margin-bottom: 0cm;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://lh5.googleusercontent.com/-mV7FcIdrz5o/TXqdyW9t1lI/AAAAAAAAAaE/3efNVR-XJyM/s1600/Response+Times+Over+Time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="197" src="https://lh5.googleusercontent.com/-mV7FcIdrz5o/TXqdyW9t1lI/AAAAAAAAAaE/3efNVR-XJyM/s320/Response+Times+Over+Time.png" width="320" /></a></div><br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://lh6.googleusercontent.com/-vvhCnFH2ows/TXqd52qQeeI/AAAAAAAAAaI/6qwTgyfeseM/s1600/Response+Latencies+Over+Time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="197" src="https://lh6.googleusercontent.com/-vvhCnFH2ows/TXqd52qQeeI/AAAAAAAAAaI/6qwTgyfeseM/s320/Response+Latencies+Over+Time.png" width="320" /></a></div><br /><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">As you can see, response times when one hundred clients are downloading a file concurrently are only about 5% higher compared to the system with almost no load. Also response latencies aren't particularly harmed by increasing load. I can't push the server even further due to my limited hardware resources, but I have reasons to believe that this simple application would handle even twice as more connection: both HTTP threads and download worker thread weren't fully utilized during the whole test. This also means that we have increased our server capacity 10 times without even using all the threads!</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Hope you enjoyed this article. Of course not every use case can be scaled so easily, but next time you'll notice your servlet is mainly waiting – don't waste HTTP threads and consider servlet 3.0 asynchronous processing. And test, measure and compare! The complete application source codes are <a href="https://github.com/nurkiewicz/token-bucket">available</a> (look at different branches), including JMeter test plan.</div><div style="margin-bottom: 0cm;"><br /></div><h5 class="western">Areas of improvement</h5>There are still several places that require attention and improvement. If you want to, don't hesitate, fork, modify and test:<br /><ul><li>While profiling I discovered that in more than 80% of  executions <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">DownloadChunkTask</span> does not acquire a token and only  reschedules itself. This is an awful waste of CPU time that can be  fixed quite easily (how?)<br /></li><li>Consider opening a file and sending content length in a  worker thread rather than in an HTTP thread (before starting  asynchronous context)<br /></li><li>How can one implement global limit on top of bandwidth limits  per request? You have at least couple of choice: either limit the  size of download workers pool queue and reject executions or wrap <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;"> PerRequestTokenBucket</span> with reimplemented <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">GlobalTokenBucket</span>  (decorator pattern)<br /></li><li><span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">TokenBucket.tryTake()</span> method does clearly violate  <a href="http://en.wikipedia.org/wiki/Command-query_separation">Command-query separation</a> principle. Could you suggest how it should  look like to follow it? Why it is so hard?<br /></li><li>I am aware that my test constantly reads the same small file,  so the I/O performance impact is minimal. But in real life scenario  some caching layer would have certainly be applied on top of disk  storage. So the difference is not that big (now the application uses  very small amount of memory, lots of place for cache)<br /></li></ul><h5 class="western">Lessons learned</h5><ul><li>Loopback interface is not infinitely fast. In fact on my  machine localhost was incapable of processing more than 80 MiB/s.<br /></li><li>I don't use plain request object as a key in <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;"> bucketSizeByRequestNo</span>. First of all, there are no guarantees on  <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">equals()</span> and <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">hashCode()</span> for this interface. And more importantly –  read the next point...<br /></li><li>With servlets 3.0 when processing the request you have to  call completed() explicitly to flush and close the connection.  Obviously after calling this method request and response objects are  useless. What wasn't obvious (and I learned that the hard why) is  that Tomcat reuses request objects (pooling) and some of their  contents for subsequent connections. This means that the following  code is incorrect and dangerous, possibly resulting in  accessing/corrupting other requests' attributes or even session (?!?)<br /></li></ul><pre class="brush: java"><br />ctx.complete();<br />ctx.getRequest().getAttribute("SOME_KEY");<br /></pre><br /><br /><ul><li>EDIT: Watch out for asynchronous executions timeouts. After creating an asynchronous invocation using <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">startAsync()</span> on Tomcat by default you have 30 seconds. After that Tomcat will kill your connection and <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">getRequest()</span> will suddenly start returning <span style="font-family: &quot;Courier New&quot;,&quot;Courier&quot;,monospace;">null</span> (not very helpful). Luckily timeout can be customized and you can listen for timeout events easily (see this <a href="https://github.com/nurkiewicz/token-bucket/commit/36023cea8dbbeed7d224b5e0237b2360981b337c">commit</a>).<br /></li></ul>